<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Spark and the Minor Planet Center data part 3 | Parsecs Reach</title>
<meta name="keywords" content="spark, minor planet center, asteroids">
<meta name="description" content="In the last post we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the first post of this series we covered reading a json file which contained information about all the asteroids we know about.
This time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object.">
<meta name="author" content="Emily Selwood">
<link rel="canonical" href="https://parsecsreach.org/post/spark_and_mpc_part_3/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.979d31a757b649bf90615e658aedd46e2896dc33bf05775ea99697ce5b3e4fe4.css" integrity="sha256-l50xp1e2Sb&#43;QYV5liu3UbiiW3DO/BXdeqZaXzls&#43;T&#43;Q=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://parsecsreach.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://parsecsreach.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://parsecsreach.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://parsecsreach.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://parsecsreach.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Spark and the Minor Planet Center data part 3" />
<meta property="og:description"
  content="In the last post we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the first post of this series we covered reading a json file which contained information about all the asteroids we know about.
This time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://parsecsreach.org/post/spark_and_mpc_part_3/" />
<meta name="fediverse:creator" content="@emily_s@mastodon.me.uk"><meta property="article:section" content="post" />

<meta property="article:published_time" content="2017-12-05T19:39:29+00:00" />

<meta property="article:modified_time" content="2017-12-05T19:39:29+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Spark and the Minor Planet Center data part 3"/>
<meta name="twitter:description" content="In the last post we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the first post of this series we covered reading a json file which contained information about all the asteroids we know about.
This time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://parsecsreach.org/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Spark and the Minor Planet Center data part 3",
      "item": "https://parsecsreach.org/post/spark_and_mpc_part_3/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark and the Minor Planet Center data part 3",
  "name": "Spark and the Minor Planet Center data part 3",
  "description": "In the last post we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the first post of this series we covered reading a json file which contained information about all the asteroids we know about.\nThis time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object.",
  "keywords": [
    "spark", "minor planet center", "asteroids"
  ],
  "articleBody": "In the last post we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the first post of this series we covered reading a json file which contained information about all the asteroids we know about.\nThis time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object. The json file we first looked at contained the year but not days and months. The observation file has the full date down to smaller than minutes accuracy, but it does not have the orbital parameters.\nSo we need to join the two files up to get our results.\nAssumptions You have been through the previous posts in this series and understood them. You have a project already set up and able to read and process the orbit and observation files.\nSetup As you should already have the project and the two required data files you are already set up.\nDevelopment Thankfully Spark SQL makes this bit really easy. the .join() function on a data set allows you do all the kinds of joins you could do in a database. For this we need a simple inner join, which is the default option so you won’t see us having to define it in the code below.\nval joined = orbRec.join(obs, Seq(\"id\", \"id\")) There is not a lot of difference in which way around you do this from the results point of view. There may be differences in performance but that will depend on your data set. In this case with the minor planets data it doesn’t really matter which way around we do it. If you now add a joined.show(2) line you will be able to see the columns from both data sets are now next to each other.\nThere are several ways to tell Spark SQL that you want to match the two id columns up. Seq(\"id\", \"id\") is probably the simplest but you can also do something like obs.col(\"id\").equalTo(orbRec.col(\"id\")) This is really useful if you want to use something that is not a simple equals when joining.\nNow we don’t really want all those columns in the output so we can add a .select() call to clean things up a bit.\n.select( obs.col(\"id\"), col(\"Name\"), col(\"date\"), col(\"a\"), col(\"e\"), col(\"i\"), col(\"Epoch\"), col(\"H\"), col(\"G\"), col(\"Node\"), col(\"Peri\") ) Unfortunately for me at this point things went bang. I got an exception about the query plan being too big. My first attempt at fixing this was adding a .select() call to the orbRec processing chain to trim down the number of columns. This didn’t work. In fact I think it made things worse. The query plan is now longer.\nThe solution is to add checkpoints. Checkpoints allow spark sql to split the query plan in to bits. The checkpoints are written to disk and persistent. This is also useful if you need to reuse a data frame multiple times but it is expensive to compute.\nThe first thing we need to do is define where to put the checkpoint files. Just under where we create the SparkSession we need to add\nspark.sparkContext.setCheckpointDir(\"/tmp/\") This will cause spark to store its checkpoint data in /tmp you may want to put it somewhere different depending on what you are running on. Now we just need to tell spark where to checkpoint. This is done with the .checkpoint() function. I tend to find it best to checkpoint before both sides of a join and any time you are going to reuse a data frame.\nWe simply need to add the call to .checkpoint() at the end of the two chains we defined for orbRec and obs. Now when we run joined.show(2) we should get something like\n+---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+ | id|Name| date| a| e| i| Epoch| H| G| Node| Peri| +---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+ |1995 SR42|null|1995-09-20T09:21:27|2.3527416|0.1811627|2.01897|2449980.5|19.0|0.15|117.61112|171.74016| | 1996 LW|null|1996-06-09T07:21:53|2.1578017|0.2848271|7.64274|2450240.5|19.5|0.15|194.61088| 128.2009| +---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+ only showing top 2 rows Now the only thing left to do is write the data back out to disk. Here we will run into one of the interesting design problems of running spark locally.\nSaving data as a csv file is as easy as reading a file. You just need a write rather than a read\njoined.write.mode(SaveMode.Overwrite).csv(\"output\") Note the lack of file extension on the target name. This is because output will be a folder and not a file. Under the hood spark will split the data frame into partitions. These are basically groups of rows that will all be on the same machine in a clustered environment. In our environment every thing is on the one machine, however in a cluster you would get one output directory on each machine with the partitions that were on that machine written out.\nAfter running the program, inside the output folder you will find a large number of files. There will be a _SUCCESS file created when every thing has finished. There will be one part-*.csv file and one .part-*.csv.crc file per partition in your data. The file ending in .crc is a checksum allowing you to verify that the data has written correctly if you need it. The files ending in .csv will be all the data you asked Spark SQL to write out.\nThe last step is to join every thing up. I find this easiest to do from the command line. (I’m on a linux machine with bash)\ncat output/part-*.csv \u003e output.csv This will generate an output.csv file which is all the parts joined together. Note that you will not be able to control the order of the rows using this.\nAnd there we go. We now have the orbital elements for all the un-numbered objects with the date of their first observation. In this part we have learnt how to join to data frames together and how to output data. This part has been a bit shorter than the others but joins every thing together. (Pun intended)\nYou can find all the code for this project on my github\nI hope this is useful to you. If you have any questions please let me know on twitter.\n",
  "wordCount" : "1045",
  "inLanguage": "en",
  "datePublished": "2017-12-05T19:39:29Z",
  "dateModified": "2017-12-05T19:39:29Z",
  "author":{
    "@type": "Person",
    "name": "Emily Selwood"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://parsecsreach.org/post/spark_and_mpc_part_3/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Parsecs Reach",
    "logo": {
      "@type": "ImageObject",
      "url": "https://parsecsreach.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://parsecsreach.org" accesskey="h" title="Parsecs Reach (Alt + H)">Parsecs Reach</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a rel="me" href="https://mastodon.me.uk/@emily_s"  target="_blank" title="">
                    <span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 417.8 512" class="logo-social" width="18" height="18"><path d="M417.8 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.5-28.4-290.4 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54-.6-4.4-.9-9-.9-13.9 85.6 20.9 158.6 9.1 178.7 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6V190.1c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H75.1c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z"/></svg></span>
                    
                </a>
            </li>
            <li>
                <a rel="" href="https://github.com/emilyselwood"  target="_blank" title="">
                    <span><svg width="18" height="18" class="logo-social" stroke="none" viewBox="0 0 11.493062 11.209467" xmlns="http://www.w3.org/2000/svg"><path d="M 5.746044,0 C 2.572808,0 0,2.572808 0,5.74675 c 0,2.538941 1.646414,4.69265 3.929944,5.452886 0.287514,0.05256 0.392289,-0.124883 0.392289,-0.277283 0,-0.136173 -0.0049,-0.49777 -0.0078,-0.977195 C 2.715997,10.292291 2.378741,9.174691 2.378741,9.174691 2.117333,8.511116 1.740566,8.334375 1.740566,8.334375 1.218808,7.977716 1.780076,7.984772 1.780076,7.984772 2.356868,8.025692 2.660257,8.577086 2.660257,8.577086 3.172843,9.45515 4.005398,9.201503 4.332776,9.054747 4.384986,8.683272 4.533154,8.429978 4.697548,8.286397 3.421551,8.141405 2.079937,7.648222 2.079937,5.446183 c 0,-0.627239 0.224014,-1.140178 0.591609,-1.541991 -0.05927,-0.145345 -0.25647,-0.729545 0.05609,-1.520825 0,0 0.4826,-0.154517 1.580445,0.589138 C 4.766339,2.845153 5.258111,2.7813 5.746709,2.779183 6.2346,2.781283 6.726372,2.845153 7.185336,2.972505 8.282475,2.22885 8.764017,2.383367 8.764017,2.383367 c 0.313619,0.79128 0.116416,1.37548 0.05715,1.520825 0.3683,0.401813 0.590903,0.914752 0.590903,1.541991 0,2.207683 -1.343731,2.693458 -2.623961,2.835628 0.206375,0.177447 0.390172,0.528108 0.390172,1.06433 0,0.767998 -0.0071,1.387828 -0.0071,1.576212 0,0.153811 0.103364,0.332669 0.395111,0.276577 2.281767,-0.761647 3.92677,-2.913944 3.92677,-5.45218 C 11.493062,2.572808 8.919901,0 5.745959,0"/></svg></span>
                    
                </a>
            </li>
            <li>
                <a rel="" href="https://parsecsreach.org/projects"  target="_self" title="Projects">
                    <span>Projects</span>
                    
                </a>
            </li>
            <li>
                <a rel="" href="https://parsecsreach.org/"  target="_self" title="Home">
                    <span>Home</span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Spark and the Minor Planet Center data part 3
    </h1>
    <div class="post-meta"><span title='2017-12-05 19:39:29 +0000 UTC'>2017 December 5</span>&nbsp;·&nbsp;Emily Selwood

</div>
  </header> 
  <div class="post-content"><p>In the <a href="/post/spark_and_mpc_part_2">last post</a> we read the minor planet center observation file. This was a fixed width text file. We only pulled a couple of columns out of it, but we learnt to use User Defined Functions, groupBy and select. In the <a href="/post/spark_and_mpc">first post</a> of this series we covered reading a json file which contained information about all the asteroids we know about.</p>
<p>This time we are going to join the two data sets together and finally solve our original problem, which was to find the full date of the earliest observation of each un-numbered object. The json file we first looked at contained the year but not days and months. The observation file has the full date down to smaller than minutes accuracy, but it does not have the orbital parameters.</p>
<p>So we need to join the two files up to get our results.</p>
<h1 id="assumptions">Assumptions<a hidden class="anchor" aria-hidden="true" href="#assumptions">#</a></h1>
<p>You have been through the previous posts in this series and understood them. You have a project already set up and able to read and process the orbit and observation files.</p>
<h1 id="setup">Setup<a hidden class="anchor" aria-hidden="true" href="#setup">#</a></h1>
<p>As you should already have the project and the two required data files you are already set up.</p>
<h1 id="development">Development<a hidden class="anchor" aria-hidden="true" href="#development">#</a></h1>
<p>Thankfully Spark SQL makes this bit really easy. the <code>.join()</code> function on a data set allows you do all the kinds of joins you could do in a database. For this we need a simple inner join, which is the default option so you won&rsquo;t see us having to define it in the code below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="color:#66d9ef">val</span> joined <span style="color:#66d9ef">=</span> orbRec<span style="color:#f92672">.</span>join<span style="color:#f92672">(</span>obs<span style="color:#f92672">,</span> <span style="color:#a6e22e">Seq</span><span style="color:#f92672">(</span><span style="color:#e6db74">&#34;id&#34;</span><span style="color:#f92672">,</span> <span style="color:#e6db74">&#34;id&#34;</span><span style="color:#f92672">))</span>
</span></span></code></pre></div><p>There is not a lot of difference in which way around you do this from the results point of view. There may be differences in performance but that will depend on your data set. In this case with the minor planets data it doesn&rsquo;t really matter which way around we do it. If you now add a <code>joined.show(2)</code> line you will be able to see the columns from both data sets are now next to each other.</p>
<p>There are several ways to tell Spark SQL that you want to match the two id columns up. <code>Seq(&quot;id&quot;, &quot;id&quot;)</code> is probably the simplest but you can also do something like <code>obs.col(&quot;id&quot;).equalTo(orbRec.col(&quot;id&quot;))</code> This is really useful if you want to use something that is not a simple equals when joining.</p>
<p>Now we don&rsquo;t really want all those columns in the output so we can add a <code>.select()</code> call to clean things up a bit.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span>  <span style="color:#f92672">.</span>select<span style="color:#f92672">(</span>
</span></span><span style="display:flex;"><span>    obs<span style="color:#f92672">.</span>col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;id&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Name&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;date&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;a&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;e&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;i&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Epoch&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;H&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;G&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Node&#34;</span><span style="color:#f92672">),</span>
</span></span><span style="display:flex;"><span>    col<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Peri&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Unfortunately for me at this point things went bang. I got an exception about the query plan being too big. My first attempt at fixing this was adding a <code>.select()</code> call to the orbRec processing chain to trim down the number of columns. This didn&rsquo;t work. In fact I think it made things worse. The query plan is now longer.</p>
<p>The solution is to add checkpoints. Checkpoints allow spark sql to split the query plan in to bits. The checkpoints are written to disk and persistent. This is also useful if you need to reuse a data frame multiple times but it is expensive to compute.</p>
<p>The first thing we need to do is define where to put the checkpoint files. Just under where we create the <code>SparkSession</code> we need to add</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>sparkContext<span style="color:#f92672">.</span>setCheckpointDir<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;/tmp/&#34;</span><span style="color:#f92672">)</span>
</span></span></code></pre></div><p>This will cause spark to store its checkpoint data in <code>/tmp</code> you may want to put it somewhere different depending on what you are running on. Now we just need to tell spark where to checkpoint. This is done with the <code>.checkpoint()</code> function. I tend to find it best to checkpoint before both sides of a join and any time you are going to reuse a data frame.</p>
<p>We simply need to add the call to <code>.checkpoint()</code> at the end of the two chains we defined for <code>orbRec</code> and <code>obs</code>. Now when we run <code>joined.show(2)</code> we should get something like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>+---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+
</span></span><span style="display:flex;"><span>|       id|Name|               date|        a|        e|      i|    Epoch|   H|   G|     Node|     Peri|
</span></span><span style="display:flex;"><span>+---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+
</span></span><span style="display:flex;"><span>|1995 SR42|null|1995-09-20T09:21:27|2.3527416|0.1811627|2.01897|2449980.5|19.0|0.15|117.61112|171.74016|
</span></span><span style="display:flex;"><span>|  1996 LW|null|1996-06-09T07:21:53|2.1578017|0.2848271|7.64274|2450240.5|19.5|0.15|194.61088| 128.2009|
</span></span><span style="display:flex;"><span>+---------+----+-------------------+---------+---------+-------+---------+----+----+---------+---------+
</span></span><span style="display:flex;"><span>only showing top 2 rows
</span></span></code></pre></div><p>Now the only thing left to do is write the data back out to disk. Here we will run into one of the interesting design problems of running spark locally.</p>
<p>Saving data as a csv file is as easy as reading a file. You just need a <code>write</code> rather than a <code>read</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span>  joined<span style="color:#f92672">.</span>write<span style="color:#f92672">.</span>mode<span style="color:#f92672">(</span><span style="color:#a6e22e">SaveMode</span><span style="color:#f92672">.</span><span style="color:#a6e22e">Overwrite</span><span style="color:#f92672">).</span>csv<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;output&#34;</span><span style="color:#f92672">)</span>
</span></span></code></pre></div><p>Note the lack of file extension on the target name. This is because <code>output</code> will be a folder and not a file. Under the hood spark will split the data frame into partitions. These are basically groups of rows that will all be on the same machine in a clustered environment. In our environment every thing is on the one machine, however in a cluster you would get one <code>output</code> directory on each machine with the partitions that were on that machine written out.</p>
<p>After running the program, inside the <code>output</code> folder you will find a large number of files. There will be a <code>_SUCCESS</code> file created when every thing has finished. There will be one <code>part-*.csv</code> file and one <code>.part-*.csv.crc</code> file per partition in your data. The file ending in .crc is a checksum allowing you to verify that the data has written correctly if you need it. The files ending in .csv will be all the data you asked Spark SQL to write out.</p>
<p>The last step is to join every thing up. I find this easiest to do from the command line. (I&rsquo;m on a linux machine with bash)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span>cat output<span style="color:#f92672">/</span>part<span style="color:#f92672">-*.</span>csv <span style="color:#f92672">&gt;</span> output<span style="color:#f92672">.</span>csv
</span></span></code></pre></div><p>This will generate an output.csv file which is all the parts joined together. Note that you will not be able to control the order of the rows using this.</p>
<p>And there we go. We now have the orbital elements for all the un-numbered objects with the date of their first observation. In this part we have learnt how to join to data frames together and how to output data. This part has been a bit shorter than the others but joins every thing together. (Pun intended)</p>
<p>You can find all the code for this <a href="https://github.com/wselwood/orbitdates">project on my github</a></p>
<p>I hope this is useful to you. If you have any questions please let me know on twitter.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://parsecsreach.org/tags/spark/">spark</a></li>
      <li><a href="https://parsecsreach.org/tags/minor-planet-center/">minor planet center</a></li>
      <li><a href="https://parsecsreach.org/tags/asteroids/">asteroids</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://parsecsreach.org/post/flatroofs_with_qgis/">
    <span class="title">« Prev</span>
    <br>
    <span>Finding flatroofs with QGIS</span>
  </a>
  <a class="next" href="https://parsecsreach.org/post/spark_and_mpc_part_2/">
    <span class="title">Next »</span>
    <br>
    <span>Spark and the Minor Planet Center data part 2</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>All rights reserved - 2022</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
